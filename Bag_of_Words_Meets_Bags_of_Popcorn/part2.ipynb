{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5c0c5e",
   "metadata": {},
   "source": [
    "### 第二部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353ac04",
   "metadata": {},
   "source": [
    "训练前的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3bbed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews,       25000 labeled test reviews,       and 50000 unlabeled train reviews.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "train = pd.read_csv(\"../dataset/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../dataset/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"../dataset/unlabeledTrainData.tsv\", header=0, \\\n",
    "                              delimiter=\"\\t\", quoting=3)\n",
    "# 验证数据\n",
    "print(f\"Read {train['review'].size} labeled train reviews, \\\n",
    "      {test['review'].size} labeled test reviews, \\\n",
    "      and {unlabeled_train['review'].size} unlabeled train reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11efada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'good', 'friend', '111', '234']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "d:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "# 类似part1, 我们需要清理数据\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Remove non-letters&number\n",
    "    # 教程说最好连数字也不要删除， 只需要稍微修改正则表达式的匹配规则即可\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # 5. Return a list of words\n",
    "    return words\n",
    "s = \"We are good friend 111 234.\"\n",
    "print(review_to_wordlist(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe18fce",
   "metadata": {},
   "source": [
    "Word2Vec的需要的输入格式是以list为单位的句子, 我们采用nltk中的punkt来分割句子.请务必区分list.append()和 += 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c1892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "# 下载完后注释掉\n",
    "nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4ea9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\ANACONDA\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44151431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "# 查看一下处理后的结果\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084d9db",
   "metadata": {},
   "source": [
    "和part1的结果相比, part2的结果中含有stopwords, 并保留有相对顺序说明words2vec在分析时考虑了上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47601498",
   "metadata": {},
   "source": [
    "确定参数并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78723af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:10:20,991 : INFO : collecting all words and their counts\n",
      "2025-04-14 17:10:20,992 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-14 17:10:21,027 : INFO : PROGRESS: at sentence #10000, processed 227240 words, keeping 18038 word types\n",
      "2025-04-14 17:10:21,060 : INFO : PROGRESS: at sentence #20000, processed 454577 words, keeping 25324 word types\n",
      "2025-04-14 17:10:21,095 : INFO : PROGRESS: at sentence #30000, processed 675275 words, keeping 30478 word types\n",
      "2025-04-14 17:10:21,131 : INFO : PROGRESS: at sentence #40000, processed 903015 words, keeping 34863 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:10:21,166 : INFO : PROGRESS: at sentence #50000, processed 1123504 words, keeping 38329 word types\n",
      "2025-04-14 17:10:21,205 : INFO : PROGRESS: at sentence #60000, processed 1346265 words, keeping 41338 word types\n",
      "2025-04-14 17:10:21,241 : INFO : PROGRESS: at sentence #70000, processed 1570739 words, keeping 43986 word types\n",
      "2025-04-14 17:10:21,279 : INFO : PROGRESS: at sentence #80000, processed 1791249 words, keeping 46400 word types\n",
      "2025-04-14 17:10:21,316 : INFO : PROGRESS: at sentence #90000, processed 2016723 words, keeping 48869 word types\n",
      "2025-04-14 17:10:21,354 : INFO : PROGRESS: at sentence #100000, processed 2239896 words, keeping 50980 word types\n",
      "2025-04-14 17:10:21,388 : INFO : PROGRESS: at sentence #110000, processed 2460901 words, keeping 52890 word types\n",
      "2025-04-14 17:10:21,426 : INFO : PROGRESS: at sentence #120000, processed 2684304 words, keeping 54967 word types\n",
      "2025-04-14 17:10:21,465 : INFO : PROGRESS: at sentence #130000, processed 2911246 words, keeping 56741 word types\n",
      "2025-04-14 17:10:21,500 : INFO : PROGRESS: at sentence #140000, processed 3125257 words, keeping 58290 word types\n",
      "2025-04-14 17:10:21,539 : INFO : PROGRESS: at sentence #150000, processed 3352187 words, keeping 60038 word types\n",
      "2025-04-14 17:10:21,575 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 61632 word types\n",
      "2025-04-14 17:10:21,616 : INFO : PROGRESS: at sentence #170000, processed 3800671 words, keeping 63128 word types\n",
      "2025-04-14 17:10:21,654 : INFO : PROGRESS: at sentence #180000, processed 4022558 words, keeping 64575 word types\n",
      "2025-04-14 17:10:21,694 : INFO : PROGRESS: at sentence #190000, processed 4249063 words, keeping 65900 word types\n",
      "2025-04-14 17:10:21,737 : INFO : PROGRESS: at sentence #200000, processed 4474464 words, keeping 67217 word types\n",
      "2025-04-14 17:10:21,781 : INFO : PROGRESS: at sentence #210000, processed 4697223 words, keeping 68546 word types\n",
      "2025-04-14 17:10:21,817 : INFO : PROGRESS: at sentence #220000, processed 4923567 words, keeping 69884 word types\n",
      "2025-04-14 17:10:21,854 : INFO : PROGRESS: at sentence #230000, processed 5147437 words, keeping 71164 word types\n",
      "2025-04-14 17:10:21,889 : INFO : PROGRESS: at sentence #240000, processed 5376270 words, keeping 72402 word types\n",
      "2025-04-14 17:10:21,926 : INFO : PROGRESS: at sentence #250000, processed 5591638 words, keeping 73620 word types\n",
      "2025-04-14 17:10:21,963 : INFO : PROGRESS: at sentence #260000, processed 5812901 words, keeping 74778 word types\n",
      "2025-04-14 17:10:22,002 : INFO : PROGRESS: at sentence #270000, processed 6035538 words, keeping 76111 word types\n",
      "2025-04-14 17:10:22,043 : INFO : PROGRESS: at sentence #280000, processed 6262672 words, keeping 77737 word types\n",
      "2025-04-14 17:10:22,087 : INFO : PROGRESS: at sentence #290000, processed 6487099 words, keeping 79229 word types\n",
      "2025-04-14 17:10:22,133 : INFO : PROGRESS: at sentence #300000, processed 6713049 words, keeping 80594 word types\n",
      "2025-04-14 17:10:22,168 : INFO : PROGRESS: at sentence #310000, processed 6939616 words, keeping 81947 word types\n",
      "2025-04-14 17:10:22,205 : INFO : PROGRESS: at sentence #320000, processed 7165783 words, keeping 83307 word types\n",
      "2025-04-14 17:10:22,245 : INFO : PROGRESS: at sentence #330000, processed 7388849 words, keeping 84568 word types\n",
      "2025-04-14 17:10:22,284 : INFO : PROGRESS: at sentence #340000, processed 7619625 words, keeping 85844 word types\n",
      "2025-04-14 17:10:22,322 : INFO : PROGRESS: at sentence #350000, processed 7844224 words, keeping 87024 word types\n",
      "2025-04-14 17:10:22,364 : INFO : PROGRESS: at sentence #360000, processed 8066036 words, keeping 88224 word types\n",
      "2025-04-14 17:10:22,402 : INFO : PROGRESS: at sentence #370000, processed 8294461 words, keeping 89358 word types\n",
      "2025-04-14 17:10:22,439 : INFO : PROGRESS: at sentence #380000, processed 8520928 words, keeping 90551 word types\n",
      "2025-04-14 17:10:22,478 : INFO : PROGRESS: at sentence #390000, processed 8752027 words, keeping 91609 word types\n",
      "2025-04-14 17:10:22,515 : INFO : PROGRESS: at sentence #400000, processed 8976260 words, keeping 92659 word types\n",
      "2025-04-14 17:10:22,553 : INFO : PROGRESS: at sentence #410000, processed 9198885 words, keeping 93646 word types\n",
      "2025-04-14 17:10:22,589 : INFO : PROGRESS: at sentence #420000, processed 9421275 words, keeping 94699 word types\n",
      "2025-04-14 17:10:22,629 : INFO : PROGRESS: at sentence #430000, processed 9650226 words, keeping 95740 word types\n",
      "2025-04-14 17:10:22,667 : INFO : PROGRESS: at sentence #440000, processed 9878340 words, keeping 96739 word types\n",
      "2025-04-14 17:10:22,705 : INFO : PROGRESS: at sentence #450000, processed 10103333 words, keeping 97893 word types\n",
      "2025-04-14 17:10:22,745 : INFO : PROGRESS: at sentence #460000, processed 10337463 words, keeping 98968 word types\n",
      "2025-04-14 17:10:22,784 : INFO : PROGRESS: at sentence #470000, processed 10566647 words, keeping 99845 word types\n",
      "2025-04-14 17:10:22,822 : INFO : PROGRESS: at sentence #480000, processed 10788293 words, keeping 100796 word types\n",
      "2025-04-14 17:10:22,861 : INFO : PROGRESS: at sentence #490000, processed 11016456 words, keeping 101878 word types\n",
      "2025-04-14 17:10:22,898 : INFO : PROGRESS: at sentence #500000, processed 11239432 words, keeping 102801 word types\n",
      "2025-04-14 17:10:22,935 : INFO : PROGRESS: at sentence #510000, processed 11466015 words, keeping 103750 word types\n",
      "2025-04-14 17:10:22,974 : INFO : PROGRESS: at sentence #520000, processed 11690736 words, keeping 104669 word types\n",
      "2025-04-14 17:10:23,013 : INFO : PROGRESS: at sentence #530000, processed 11916464 words, keeping 105502 word types\n",
      "2025-04-14 17:10:23,052 : INFO : PROGRESS: at sentence #540000, processed 12142447 words, keeping 106401 word types\n",
      "2025-04-14 17:10:23,090 : INFO : PROGRESS: at sentence #550000, processed 12369300 words, keeping 107290 word types\n",
      "2025-04-14 17:10:23,128 : INFO : PROGRESS: at sentence #560000, processed 12591871 words, keeping 108175 word types\n",
      "2025-04-14 17:10:23,168 : INFO : PROGRESS: at sentence #570000, processed 12822238 words, keeping 108982 word types\n",
      "2025-04-14 17:10:23,206 : INFO : PROGRESS: at sentence #580000, processed 13045003 words, keeping 109872 word types\n",
      "2025-04-14 17:10:23,244 : INFO : PROGRESS: at sentence #590000, processed 13271738 words, keeping 110743 word types\n",
      "2025-04-14 17:10:23,282 : INFO : PROGRESS: at sentence #600000, processed 13495259 words, keeping 111494 word types\n",
      "2025-04-14 17:10:23,318 : INFO : PROGRESS: at sentence #610000, processed 13717559 words, keeping 112396 word types\n",
      "2025-04-14 17:10:23,359 : INFO : PROGRESS: at sentence #620000, processed 13945321 words, keeping 113170 word types\n",
      "2025-04-14 17:10:23,397 : INFO : PROGRESS: at sentence #630000, processed 14170897 words, keeping 113969 word types\n",
      "2025-04-14 17:10:23,436 : INFO : PROGRESS: at sentence #640000, processed 14392921 words, keeping 114809 word types\n",
      "2025-04-14 17:10:23,477 : INFO : PROGRESS: at sentence #650000, processed 14620123 words, keeping 115618 word types\n",
      "2025-04-14 17:10:23,517 : INFO : PROGRESS: at sentence #660000, processed 14844191 words, keeping 116394 word types\n",
      "2025-04-14 17:10:23,554 : INFO : PROGRESS: at sentence #670000, processed 15068851 words, keeping 117115 word types\n",
      "2025-04-14 17:10:23,593 : INFO : PROGRESS: at sentence #680000, processed 15295014 words, keeping 117840 word types\n",
      "2025-04-14 17:10:23,629 : INFO : PROGRESS: at sentence #690000, processed 15518432 words, keeping 118637 word types\n",
      "2025-04-14 17:10:23,667 : INFO : PROGRESS: at sentence #700000, processed 15748360 words, keeping 119468 word types\n",
      "2025-04-14 17:10:23,705 : INFO : PROGRESS: at sentence #710000, processed 15972643 words, keeping 120130 word types\n",
      "2025-04-14 17:10:23,744 : INFO : PROGRESS: at sentence #720000, processed 16199229 words, keeping 120765 word types\n",
      "2025-04-14 17:10:23,782 : INFO : PROGRESS: at sentence #730000, processed 16426860 words, keeping 121513 word types\n",
      "2025-04-14 17:10:23,823 : INFO : PROGRESS: at sentence #740000, processed 16649236 words, keeping 122242 word types\n",
      "2025-04-14 17:10:23,859 : INFO : PROGRESS: at sentence #750000, processed 16868896 words, keeping 122892 word types\n",
      "2025-04-14 17:10:23,896 : INFO : PROGRESS: at sentence #760000, processed 17089573 words, keeping 123538 word types\n",
      "2025-04-14 17:10:23,935 : INFO : PROGRESS: at sentence #770000, processed 17318060 words, keeping 124325 word types\n",
      "2025-04-14 17:10:23,975 : INFO : PROGRESS: at sentence #780000, processed 17549563 words, keeping 125051 word types\n",
      "2025-04-14 17:10:24,013 : INFO : PROGRESS: at sentence #790000, processed 17777883 words, keeping 125739 word types\n",
      "2025-04-14 17:10:24,035 : INFO : collected 126186 word types from a corpus of 17901685 raw words and 795538 sentences\n",
      "2025-04-14 17:10:24,036 : INFO : Creating a fresh vocabulary\n",
      "2025-04-14 17:10:24,116 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16731 unique words (13.26% of original 126186, drops 109455)', 'datetime': '2025-04-14T17:10:24.116787', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-14 17:10:24,117 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17335523 word corpus (96.84% of original 17901685, drops 566162)', 'datetime': '2025-04-14T17:10:24.117785', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-14 17:10:24,203 : INFO : deleting the raw counts dictionary of 126186 items\n",
      "2025-04-14 17:10:24,206 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-04-14 17:10:24,207 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12862720.377205513 word corpus (74.2%% of prior 17335523)', 'datetime': '2025-04-14T17:10:24.207545', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-14 17:10:24,349 : INFO : estimated required memory for 16731 words and 300 dimensions: 48519900 bytes\n",
      "2025-04-14 17:10:24,350 : INFO : resetting layer weights\n",
      "2025-04-14 17:10:24,368 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-14T17:10:24.368115', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2025-04-14 17:10:24,369 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16731 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-04-14T17:10:24.369113', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2025-04-14 17:10:25,375 : INFO : EPOCH 0 - PROGRESS: at 10.52% examples, 1344230 words/s, in_qsize 7, out_qsize 1\n",
      "2025-04-14 17:10:26,376 : INFO : EPOCH 0 - PROGRESS: at 21.00% examples, 1338709 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:27,385 : INFO : EPOCH 0 - PROGRESS: at 32.11% examples, 1362513 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:28,387 : INFO : EPOCH 0 - PROGRESS: at 43.05% examples, 1374299 words/s, in_qsize 8, out_qsize 1\n",
      "2025-04-14 17:10:29,388 : INFO : EPOCH 0 - PROGRESS: at 54.12% examples, 1384333 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:30,392 : INFO : EPOCH 0 - PROGRESS: at 64.96% examples, 1387084 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:31,394 : INFO : EPOCH 0 - PROGRESS: at 75.54% examples, 1383227 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:32,394 : INFO : EPOCH 0 - PROGRESS: at 86.02% examples, 1378851 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:33,395 : INFO : EPOCH 0 - PROGRESS: at 96.90% examples, 1380731 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:33,690 : INFO : EPOCH 0: training on 17901685 raw words (12863633 effective words) took 9.3s, 1380698 effective words/s\n",
      "2025-04-14 17:10:34,697 : INFO : EPOCH 1 - PROGRESS: at 10.41% examples, 1329300 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:35,703 : INFO : EPOCH 1 - PROGRESS: at 21.23% examples, 1349066 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:36,707 : INFO : EPOCH 1 - PROGRESS: at 31.42% examples, 1332552 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:37,712 : INFO : EPOCH 1 - PROGRESS: at 41.95% examples, 1336496 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:38,718 : INFO : EPOCH 1 - PROGRESS: at 52.81% examples, 1347076 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:39,721 : INFO : EPOCH 1 - PROGRESS: at 63.69% examples, 1357298 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:40,723 : INFO : EPOCH 1 - PROGRESS: at 74.24% examples, 1357772 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:41,727 : INFO : EPOCH 1 - PROGRESS: at 85.11% examples, 1362112 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:42,728 : INFO : EPOCH 1 - PROGRESS: at 94.10% examples, 1339003 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:43,347 : INFO : EPOCH 1: training on 17901685 raw words (12859227 effective words) took 9.7s, 1332108 effective words/s\n",
      "2025-04-14 17:10:44,354 : INFO : EPOCH 2 - PROGRESS: at 8.56% examples, 1094728 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:45,359 : INFO : EPOCH 2 - PROGRESS: at 16.79% examples, 1069290 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:46,362 : INFO : EPOCH 2 - PROGRESS: at 25.07% examples, 1065484 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:47,362 : INFO : EPOCH 2 - PROGRESS: at 34.30% examples, 1092874 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:48,364 : INFO : EPOCH 2 - PROGRESS: at 44.01% examples, 1124829 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:49,366 : INFO : EPOCH 2 - PROGRESS: at 53.58% examples, 1142460 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:50,369 : INFO : EPOCH 2 - PROGRESS: at 62.16% examples, 1138579 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:51,375 : INFO : EPOCH 2 - PROGRESS: at 71.90% examples, 1152369 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:52,378 : INFO : EPOCH 2 - PROGRESS: at 82.12% examples, 1169726 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:53,382 : INFO : EPOCH 2 - PROGRESS: at 92.23% examples, 1182660 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:10:54,177 : INFO : EPOCH 2: training on 17901685 raw words (12862256 effective words) took 10.8s, 1188280 effective words/s\n",
      "2025-04-14 17:10:55,183 : INFO : EPOCH 3 - PROGRESS: at 9.86% examples, 1261405 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:56,197 : INFO : EPOCH 3 - PROGRESS: at 19.93% examples, 1263746 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:57,203 : INFO : EPOCH 3 - PROGRESS: at 29.15% examples, 1234591 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:58,208 : INFO : EPOCH 3 - PROGRESS: at 39.42% examples, 1252504 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:10:59,213 : INFO : EPOCH 3 - PROGRESS: at 49.81% examples, 1270266 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:11:00,212 : INFO : EPOCH 3 - PROGRESS: at 60.16% examples, 1281897 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:01,215 : INFO : EPOCH 3 - PROGRESS: at 70.52% examples, 1289138 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:11:02,219 : INFO : EPOCH 3 - PROGRESS: at 80.56% examples, 1288804 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:03,230 : INFO : EPOCH 3 - PROGRESS: at 90.62% examples, 1288223 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:04,156 : INFO : EPOCH 3: training on 17901685 raw words (12863066 effective words) took 10.0s, 1289649 effective words/s\n",
      "2025-04-14 17:11:05,164 : INFO : EPOCH 4 - PROGRESS: at 9.86% examples, 1259455 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:06,166 : INFO : EPOCH 4 - PROGRESS: at 19.93% examples, 1270779 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:11:07,173 : INFO : EPOCH 4 - PROGRESS: at 29.93% examples, 1272470 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:08,175 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 1253224 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:09,181 : INFO : EPOCH 4 - PROGRESS: at 49.21% examples, 1257752 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:10,187 : INFO : EPOCH 4 - PROGRESS: at 59.16% examples, 1261698 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:11,192 : INFO : EPOCH 4 - PROGRESS: at 69.18% examples, 1265190 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:12,195 : INFO : EPOCH 4 - PROGRESS: at 78.78% examples, 1260700 words/s, in_qsize 7, out_qsize 0\n",
      "2025-04-14 17:11:13,197 : INFO : EPOCH 4 - PROGRESS: at 88.90% examples, 1265398 words/s, in_qsize 8, out_qsize 0\n",
      "2025-04-14 17:11:14,207 : INFO : EPOCH 4 - PROGRESS: at 98.93% examples, 1266902 words/s, in_qsize 6, out_qsize 1\n",
      "2025-04-14 17:11:14,303 : INFO : EPOCH 4: training on 17901685 raw words (12864800 effective words) took 10.1s, 1268580 effective words/s\n",
      "2025-04-14 17:11:14,303 : INFO : Word2Vec lifecycle event {'msg': 'training on 89508425 raw words (64312982 effective words) took 49.9s, 1287939 effective words/s', 'datetime': '2025-04-14T17:11:14.303869', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2025-04-14 17:11:14,304 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16731, vector_size=300, alpha=0.025>', 'datetime': '2025-04-14T17:11:14.304841', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "d:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "2025-04-14 17:11:14,320 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2025-04-14 17:11:14,323 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-04-14T17:11:14.323788', 'gensim': '4.2.0', 'python': '3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2025-04-14 17:11:14,324 : INFO : not storing attribute cum_table\n",
      "2025-04-14 17:11:14,358 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56b746",
   "metadata": {},
   "source": [
    "测试我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f47e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回最不相识的\n",
    "# 新版的gensim将词向量相关内容移动到.wv中\n",
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69458a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们的模型对细微的差异很敏感\n",
    "# 如: 能够区分城市和国家\n",
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630bb769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7552964687347412),\n",
       " ('atrocious', 0.7382639050483704),\n",
       " ('horrible', 0.7245572805404663),\n",
       " ('abysmal', 0.7207901477813721),\n",
       " ('dreadful', 0.7166836857795715),\n",
       " ('appalling', 0.7005763649940491),\n",
       " ('horrid', 0.6964068412780762),\n",
       " ('horrendous', 0.6856689453125),\n",
       " ('amateurish', 0.6398955583572388),\n",
       " ('laughable', 0.6197744607925415)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc30271",
   "metadata": {},
   "source": [
    "### 第三部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93928a5f",
   "metadata": {},
   "source": [
    "在这一部分, 我们尝试如何将训练好的词向量运用到监督学习中."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca494b",
   "metadata": {},
   "source": [
    "word2vec的词向量保存在wv.vectors中.其中行数为字典中的word number, 列数为训练模型时规定的num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed182320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16731, 300)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "print(type(model.wv.vectors))\n",
    "print(model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0515a3a",
   "metadata": {},
   "source": [
    "可以查看特定单词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f93a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01041056  0.04125478  0.02076882  0.08313817 -0.02562958 -0.08931556\n",
      "  0.0251859   0.1030851   0.07478915 -0.07058468  0.05776194  0.03171209\n",
      "  0.01501744  0.09055397 -0.04950409 -0.05708684  0.04555706 -0.11122721\n",
      " -0.00563609 -0.0472794  -0.02630693 -0.00483534  0.007275    0.07415794\n",
      "  0.06638899 -0.09303596 -0.05580951  0.0193401  -0.08157014 -0.00214469\n",
      "  0.0970979  -0.01206447 -0.03748423 -0.03147227 -0.01008049  0.05060023\n",
      " -0.00662906 -0.02343504  0.03069458 -0.01843819 -0.02163189  0.04567805\n",
      "  0.10800102 -0.04625378 -0.06781906  0.04858699  0.04128491 -0.02616478\n",
      "  0.01666732  0.06230603  0.07332691 -0.02478444 -0.02854712 -0.0251361\n",
      " -0.12528984  0.0294585   0.08240269  0.00387076  0.00197295 -0.00840849\n",
      " -0.09343222  0.08877736 -0.02080078  0.09056281 -0.08471962  0.10664986\n",
      " -0.05752569  0.04569745 -0.02267064 -0.01436201 -0.0004109  -0.0133284\n",
      "  0.05110568  0.02125193  0.01463221 -0.08360124  0.00918045  0.1312618\n",
      " -0.07167825  0.18238837  0.01082698  0.01080399 -0.00307408  0.15352403\n",
      " -0.03362697  0.0058099  -0.00679867  0.04115562  0.13869119  0.02918862\n",
      "  0.0340142   0.00315737 -0.03225258 -0.00270675  0.01827535 -0.09796732\n",
      "  0.03212215  0.03402464 -0.16967754  0.02703995 -0.00674193 -0.00447342\n",
      "  0.0731333  -0.07456087  0.04005462 -0.0002667  -0.07468382  0.02758194\n",
      "  0.0090619  -0.01823851 -0.05011959 -0.01920795  0.02708812  0.07136478\n",
      "  0.03231881  0.03331201  0.02428489  0.12360122  0.15026607 -0.13803716\n",
      " -0.01309448 -0.0456385   0.0442801  -0.02029403  0.03461892  0.01085432\n",
      "  0.10133127 -0.04015302  0.03604018 -0.00243355  0.02645969  0.04199238\n",
      " -0.06243959 -0.01020428 -0.00827823  0.01206051 -0.03782877  0.00777065\n",
      " -0.02576499  0.01961543  0.02376547 -0.07885356 -0.00298673  0.11535113\n",
      "  0.01272551  0.0319057  -0.13835117 -0.06127396  0.04346334  0.09215335\n",
      "  0.07169157 -0.10170612 -0.1626943   0.02846686 -0.00772312  0.01959884\n",
      " -0.04393383 -0.02720239 -0.07989255  0.0729265  -0.00317774  0.04360524\n",
      " -0.00054825  0.14653465 -0.09429909 -0.03856359  0.09120803 -0.05879282\n",
      "  0.01050527 -0.00931306 -0.00414591 -0.02359225 -0.08749098 -0.06067071\n",
      " -0.01675809 -0.01030784  0.01495428 -0.04039808  0.01263832 -0.0485546\n",
      "  0.0021155   0.06650142  0.06775433 -0.03360829 -0.04748092  0.01468018\n",
      " -0.00373406  0.04072396  0.06374569 -0.01158874 -0.04067626  0.03876825\n",
      " -0.0147728  -0.01759913 -0.03675999  0.03090934 -0.03214651 -0.00922127\n",
      " -0.00874451 -0.10968371 -0.06124342  0.08891585  0.03987173 -0.06998274\n",
      "  0.0191954   0.04661929  0.0086918   0.02557156  0.01290674 -0.00569685\n",
      "  0.03712103  0.03451417  0.04442197 -0.05930056 -0.01690293  0.06212823\n",
      " -0.08458471 -0.01927203  0.02869839  0.00412373  0.08941756  0.07109088\n",
      " -0.08370332 -0.00510026 -0.10385467 -0.00882612 -0.09771059 -0.01019917\n",
      " -0.01541482 -0.05130009 -0.04000559 -0.0347365  -0.02921376 -0.0468189\n",
      "  0.02605488 -0.00142964  0.01817323  0.0398397  -0.00987864 -0.04690522\n",
      "  0.06464022  0.06638181 -0.05157768 -0.00966934 -0.02490065  0.02010264\n",
      " -0.00488885 -0.04539018 -0.02241181  0.05047741  0.0158915   0.00759904\n",
      "  0.03013315 -0.04212551 -0.00097868 -0.04869288  0.12175068  0.03002141\n",
      " -0.01195365 -0.05017504 -0.04065492 -0.00914828  0.11323939 -0.03872979\n",
      "  0.01188927  0.00421438  0.035778    0.06454977  0.04778272 -0.00950343\n",
      "  0.01615867  0.09380767 -0.10823172 -0.05687576  0.09053861 -0.06522384\n",
      " -0.01542172 -0.03641494 -0.07044028  0.02009812  0.04641549 -0.06238726\n",
      " -0.04879562  0.04686407 -0.02444878 -0.0916272  -0.07801578  0.03713361\n",
      "  0.07054039  0.02575513 -0.0555402   0.03827184  0.08017734  0.00559131\n",
      "  0.1138956   0.09613837 -0.1015166   0.0297238  -0.03448674  0.00494791]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"flower\"])\n",
    "print(len(model.wv[\"flower\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccdb48",
   "metadata": {},
   "source": [
    "### 从单词到段落: 尝试1,向量平均"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26762267",
   "metadata": {},
   "source": [
    "我们可以将整个review的词向量简单的相加求平均, 用平均后向量来代表整个段落."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 将review转换为word vector\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed # index2word \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec, model.wv[word])\n",
    "    \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features), dtype=\"float32\")\n",
    "    for i, review in enumerate(reviews):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Review %d of %d\".format(i, len(reviews)))\n",
    "        reviewFeatureVecs[i] = makeFeatureVec(review, model, num_features)\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "print(\"Creating average feature vecs for train reviews\")\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
