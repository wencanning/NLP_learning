{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:18.177304Z",
     "iopub.status.busy": "2025-05-08T05:55:18.176970Z",
     "iopub.status.idle": "2025-05-08T05:55:19.011375Z",
     "shell.execute_reply": "2025-05-08T05:55:19.010630Z",
     "shell.execute_reply.started": "2025-05-08T05:55:18.177276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imdb-sentiment-analysis/testData.tsv\n",
      "/kaggle/input/imdb-sentiment-analysis/labeledTrainData.tsv\n",
      "/kaggle/input/imdb-sentiment-analysis/sampleSubmission.csv\n",
      "/kaggle/input/imdb-sentiment-analysis/unlabeledTrainData.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:19.013275Z",
     "iopub.status.busy": "2025-05-08T05:55:19.012905Z",
     "iopub.status.idle": "2025-05-08T05:55:19.018616Z",
     "shell.execute_reply": "2025-05-08T05:55:19.017927Z",
     "shell.execute_reply.started": "2025-05-08T05:55:19.013255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:19.019619Z",
     "iopub.status.busy": "2025-05-08T05:55:19.019431Z",
     "iopub.status.idle": "2025-05-08T05:55:26.939210Z",
     "shell.execute_reply": "2025-05-08T05:55:26.938615Z",
     "shell.execute_reply.started": "2025-05-08T05:55:19.019604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:26.940262Z",
     "iopub.status.busy": "2025-05-08T05:55:26.939912Z",
     "iopub.status.idle": "2025-05-08T05:55:28.608086Z",
     "shell.execute_reply": "2025-05-08T05:55:28.607357Z",
     "shell.execute_reply.started": "2025-05-08T05:55:26.940243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/imdb-sentiment-analysis/labeledTrainData.tsv\",\n",
    "                header=0, \n",
    "                delimiter=\"\\t\", \n",
    "                quoting=3)\n",
    "test = pd.read_csv(\"/kaggle/input/imdb-sentiment-analysis/testData.tsv\",\n",
    "                header=0, \n",
    "                delimiter=\"\\t\", \n",
    "                quoting=3)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:28.610091Z",
     "iopub.status.busy": "2025-05-08T05:55:28.609837Z",
     "iopub.status.idle": "2025-05-08T05:55:28.732063Z",
     "shell.execute_reply": "2025-05-08T05:55:28.731292Z",
     "shell.execute_reply.started": "2025-05-08T05:55:28.610075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts = [], [], []\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    train_texts.append(review)\n",
    "    train_labels.append(train[\"sentiment\"][i])\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    test_texts.append(review)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:28.733024Z",
     "iopub.status.busy": "2025-05-08T05:55:28.732804Z",
     "iopub.status.idle": "2025-05-08T05:55:54.340515Z",
     "shell.execute_reply": "2025-05-08T05:55:54.339712Z",
     "shell.execute_reply.started": "2025-05-08T05:55:28.733007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3347c719e74ed29e0b8b1077145b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dddceaee1164843aa301799c9458521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2563ea445184179b0cee69114778117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条样本的编码:\n",
      "{'input_ids': [1, 307, 5274, 386, 2874, 3107, 281, 942, 310, 354, 791, 271, 84018, 79153, 261, 3481, 364, 293, 262, 713, 272, 9497, 309, 1135, 10493, 508, 1301, 2620, 309, 13840, 268, 349, 287, 2620, 309, 8000, 5491, 5966, 3274, 302, 2620, 309, 699, 264, 791, 285, 260, 9497, 309, 3074, 5782, 262, 37882, 2620, 309, 269, 675, 817, 371, 619, 260, 325, 280, 268, 305, 314, 1452, 261, 262, 924, 261, 263, 262, 47684, 492, 260, 273, 4352, 272, 273, 418, 280, 297, 1221, 758, 292, 278, 261, 304, 278, 490, 266, 397, 688, 7160, 758, 261, 1999, 278, 1721, 304, 6003, 260, 273, 2218, 272, 274, 295, 489, 2795, 277, 1172, 434, 10807, 287, 8910, 18289, 262, 1718, 265, 12199, 262, 37147, 261, 373, 2787, 266, 8845, 422, 285, 260, 4052, 8981, 840, 1504, 4052, 8981, 840, 1504, 21978, 9497, 309, 3074, 5782, 262, 37882, 2620, 309, 1161, 372, 282, 657, 264, 433, 261, 873, 262, 410, 470, 264, 398, 278, 269, 267, 9497, 309, 1474, 368, 358, 56700, 2620, 309, 294, 266, 938, 269, 2106, 278, 438, 266, 99285, 269, 7270, 260, 309, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "print(\"第一条样本的编码:\")\n",
    "print({k: v[0] for k, v in train_encodings.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:54.341931Z",
     "iopub.status.busy": "2025-05-08T05:55:54.341432Z",
     "iopub.status.idle": "2025-05-08T05:55:54.349465Z",
     "shell.execute_reply": "2025-05-08T05:55:54.348754Z",
     "shell.execute_reply.started": "2025-05-08T05:55:54.341898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dict.items()返回字典中所有键值对的视图对象，格式为 (key, value) 元组。\n",
    "        # item是一个字典，保存了每个键的第idx个样本\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_samples=0):\n",
    "        self.encodings = encodings\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "train_dataset = TrainDataset(train_encodings, train_labels)\n",
    "val_dataset = TrainDataset(val_encodings, val_labels)\n",
    "test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:55:54.350529Z",
     "iopub.status.busy": "2025-05-08T05:55:54.350288Z",
     "iopub.status.idle": "2025-05-08T05:56:16.030164Z",
     "shell.execute_reply": "2025-05-08T05:56:16.029564Z",
     "shell.execute_reply.started": "2025-05-08T05:55:54.350501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 05:56:00.495124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746683760.678053      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746683760.730833      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561209630f394bfb98dbc053e9c3b5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:56:16.031442Z",
     "iopub.status.busy": "2025-05-08T05:56:16.030845Z",
     "iopub.status.idle": "2025-05-08T07:29:14.544166Z",
     "shell.execute_reply": "2025-05-08T07:29:14.543543Z",
     "shell.execute_reply.started": "2025-05-08T05:56:16.031423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ee76f1ee9447a1aea1c5dc58ab6fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2500/2500 [27:18<00:00,  1.53it/s, epoch=0, train loss=0.1784, train acc=0.93, val loss=0.1304, val acc=0.95, time=1638.49]\n",
      "Epoch 1: 100%|██████████| 2500/2500 [27:16<00:00,  1.53it/s, epoch=1, train loss=0.1023, train acc=0.97, val loss=0.1361, val acc=0.95, time=1637.72]\n",
      "Epoch 2: 100%|██████████| 2500/2500 [27:16<00:00,  1.53it/s, epoch=2, train loss=0.0635, train acc=0.98, val loss=0.1630, val acc=0.95, time=1638.26]\n",
      "Predction: 100%|██████████| 1563/1563 [11:03<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    start = time.time()\n",
    "    train_loss, val_losses = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n",
    "        for batch in train_loader:\n",
    "            n += 1\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "            train_loss += loss.cpu()\n",
    "\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                'train acc': '%.2f' % (train_acc / n)\n",
    "                                })\n",
    "            pbar.update(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                m += 1\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "                val_losses += val_loss\n",
    "        end = time.time()\n",
    "        runtime = end - start\n",
    "        pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                            'train loss': '%.4f' % (train_loss.data / n),\n",
    "                            'train acc': '%.2f' % (train_acc / n),\n",
    "                            'val loss': '%.4f' % (val_losses.data / m),\n",
    "                            'val acc': '%.2f' % (val_acc / m),\n",
    "                            'time': '%.2f' % (runtime)})\n",
    "\n",
    "        # print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n",
    "        #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "test_pred = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc='Predction') as pbar:\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # test_pred.extent\n",
    "            test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "result_output.to_csv(\"deberta.csv\", index=False, quoting=3)\n",
    "logging.info('result saved!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7361453,
     "sourceId": 11727257,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
