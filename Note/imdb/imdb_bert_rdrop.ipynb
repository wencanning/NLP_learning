{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b094426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(input, target, reduction=\"sum\"):\n",
    "    input = input.float()\n",
    "    target = target.float()\n",
    "    loss = F.kl_div(F.log_softmax(input, dim=-1, dtype=torch.float32),\n",
    "                    F.softmax(target, dtype=torch.float32), reduction=reduction)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class BertScratch(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # dropout 的概率\n",
    "        # 括号允许将长条件表达式（如三元运算符）分成多行，提升可读性\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        kl_outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        kl_output = kl_outputs[1]\n",
    "        kl_output = self.dropout(kl_output)\n",
    "        kl_logits = self.classifier(kl_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            ce_loss = loss_fct(kl_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            kl_loss = (KL(logits, kl_logits, \"sum\") + KL(kl_logits, logits, \"sum\")) / 2.\n",
    "            total_loss = loss + ce_loss + kl_loss\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=total_loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5aa439",
   "metadata": {},
   "source": [
    "## KL散度函数\n",
    "\n",
    "### 关于log_softmax\n",
    "\n",
    "- softmax\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "- log_softmax\n",
    "$$\n",
    "\\log \\text{softmax}(x_i) = x_i - \\log \\sum_{j} e^{x_j}\n",
    "$$\n",
    "log_softmax即简单的对softmax加上log\n",
    "\n",
    "**为什么要这么做？**\n",
    "\n",
    "为了数值稳定性。对于原本的log_max，任然需要计算e^x，这任然可能导致溢出。但在nn.F.log_softmax内部实现中添加了**最大值平移技巧**：\n",
    "\n",
    "$$\n",
    "\\text{log\\_softmax}(x_i) = (x_i - M) - \\log \\sum_{j} e^{x_j - M}\n",
    "$$\n",
    "其中：\n",
    "- $M = \\max(x_j)$ 是输入向量 $x$ 的最大值，\n",
    "- $x_i - M$ 将输入平移至 $(-\\infty, 0]$ 范围内，\n",
    "- $\\log \\sum_{j} e^{x_j - M}$ 是数值稳定的对数求和项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c08537",
   "metadata": {},
   "source": [
    "### torch.nn.functional.kl_div\n",
    "\n",
    "KL散度的计算公式\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\sum_{i=1}^{n} P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "`torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)`\n",
    "- input和target要求是符合概率分布，且input处于log空间\n",
    "\n",
    "### KL散度的作用\n",
    "KL散度可以用于比较模型预测分布与真实分布的差异，KL散度越小代表两个分布的越相近，KL散度虽然不是一个真正的距离度量（因为它不对称），但它提供了一种有效的方式来量化分布之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd888d68",
   "metadata": {},
   "source": [
    "### KL散度在BertModel中的作用\n",
    "模型对同一输入进行两次独立的BERT前向计算，得到两组logits（logits和kl_logits）。计算两组logits之间的对称KL散度（kl_loss），衡量模型两次预测分布的一致性。KL散度通过约束两次前向传播的输出分布相似性，防止模型过拟合，提升泛化能力。​​\n",
    "\n",
    "与传统交叉熵的区别​​：交叉熵直接优化预测与标签的匹配，而KL散度优化模型内部的一致性，属于辅助损失"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
