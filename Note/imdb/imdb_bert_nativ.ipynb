{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e650f1",
   "metadata": {},
   "source": [
    "直接使用Hugging Face的transformers库原生接口加载预训练BERTBertForSequenceClassification），并基于IMDB数据集进行​​微调​​。特点是代码简洁，依赖库的封装接口，适合快速实现标准任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cd4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# sklearn.metrics提供了大量用于分类、回归、聚类等任务的评估指标，以及一些工具函数来帮助分析模型的预测结果\n",
    "# sklearn.model_selection主要用于模型选择和评估。它提供了多种方法来帮助你进行数据集划分、交叉验证、超参数调优等任务\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#注：在新版本中AdamW已经不存在，推荐版本：pip install transformers==4.36.2\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9438971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:49:00,319: INFO: running d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py--f=c:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-v369fcaf924f9af8139c5bc274a2c2467f11969760.json\n"
     ]
    }
   ],
   "source": [
    "# argv[0]返回当前文件路径\n",
    "# basename返回文件名\n",
    "program = os.path.basename(sys.argv[0])\n",
    "# 以文件名创建日志记录器对象。若记录器已存在则返回现有对象\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "# 配置日志系统的基础设置（如输出格式）\n",
    "# %(asctime)s:时间戳  %(levelname)s日志级别   %(message)s消息内容\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(r\"running %s\" % ''.join(sys.argv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8f5bb",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0659d98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledTrainDataPath = r\"D:\\workplace\\NLP_learning\\dataset\\labeledTrainData.tsv\"\n",
    "testDataPath = r\"D:\\workplace\\NLP_learning\\dataset\\testData.tsv\"\n",
    "\n",
    "# header:指定哪一行为列名\n",
    "# delimiter: 指定字段之间的分隔符(\\t for tsv and , for csv)\n",
    "# quoting: 控制引号的处理方式,3对应csv.QUOTE_NONE，表示不处理引号，将引号视为普通字符\n",
    "def ReadData(path):\n",
    "    return pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
    "train = ReadData(labeledTrainDataPath)\n",
    "test = ReadData(testDataPath)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfeb6c1",
   "metadata": {},
   "source": [
    "利用bert来tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76e7b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels, test_texts = [], [], []\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    train_texts.append(review)\n",
    "    train_labels.append(train[\"sentiment\"][i])\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    test_texts.append(review)\n",
    "\n",
    "# train_test_split会同步划分X和Y\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "# bert-base-uncased 是 Hugging Face Transformers 库中预定义的BERT模型名称\n",
    "# 它代表一个特定配置的 BERT 模型\n",
    "# uncased输入文本转为小写.适用场景:大小写无关的任务（如情感分析）\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer返回类似字典的数据结构 \n",
    "\"\"\" \n",
    "    {\n",
    "    'input_ids':      [[句子1的token IDs], [句子2的token IDs], ...],    文本转换为词典中的ID\n",
    "    'attention_mask': [[句子1的mask], [句子2的mask], ...],              标记哪些是真实token（1） vs 填充部分（0）\n",
    "    'token_type_ids': [[句子1的segment IDs], ...]                       区分句子A/B（单句任务通常全0）\n",
    "    }   \n",
    "\"\"\"\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "print(type(train_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04321d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条样本的编码:\n",
      "{'input_ids': [101, 1000, 2023, 5046, 3185, 2003, 2061, 10231, 1010, 1996, 5889, 2064, 2025, 2552, 3426, 2027, 3849, 2000, 2022, 3752, 2013, 1037, 2338, 1998, 1996, 2466, 2003, 2061, 1006, 10587, 4783, 1007, 5365, 1012, 1012, 1996, 2069, 3364, 2040, 2106, 1037, 7929, 3105, 2001, 5292, 12462, 4103, 22479, 26036, 2063, 1012, 1012, 1017, 1013, 2184, 2065, 2017, 2215, 1037, 2428, 2204, 5046, 3185, 3422, 8937, 1010, 2307, 5889, 1998, 1037, 2514, 24146, 2466, 1023, 1013, 2184, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"第一条样本的编码:\")\n",
    "print({k: v[0] for k, v in train_encodings.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3439",
   "metadata": {},
   "source": [
    "使用DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74122278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dict.items()返回字典中所有键值对的视图对象，格式为 (key, value) 元组。\n",
    "        # item是一个字典，保存了每个键的第idx个样本\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_samples=0):\n",
    "        self.encodings = encodings\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "train_dataset = TrainDataset(train_encodings, train_labels)\n",
    "val_dataset = TrainDataset(val_encodings, val_labels)\n",
    "test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91861942",
   "metadata": {},
   "source": [
    "DataLoader小总结\n",
    "\n",
    "batch的格式由两个因素决定：\n",
    "- TrainDataset.__getitem__的返回值​\n",
    "- ​DataLoader的自动批处理功能​​（将多个样本的字典按字段堆叠）\n",
    "```\n",
    "示例：假设batch_size=2，原始数据如下\n",
    "样本1: {'input_ids': [101, 2023, 3185, 102], 'attention_mask': [1,1,1,1], 'labels': 1}\n",
    "样本2: {'input_ids': [101, 1045, 2134, 102], 'attention_mask': [1,1,1,1], 'labels': 0}\n",
    "输出的batch：\n",
    "{\n",
    "    'input_ids': tensor([\n",
    "        [101, 2023, 3185, 102],  # 样本1\n",
    "        [101, 1045, 2134, 102]   # 样本2\n",
    "    ]),\n",
    "    'attention_mask': tensor([\n",
    "        [1, 1, 1, 1], \n",
    "        [1, 1, 1, 1]\n",
    "    ]),\n",
    "    'labels': tensor([1, 0])  # 样本1和样本2的标签\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c352737",
   "metadata": {},
   "source": [
    "创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# python的三元运算符 [结果1] if [条件] else [结果2]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# 在预训练BERT模型的基础上，自动追加一个适合分类任务的全连接神经网络层​​\n",
    "# 该模型默认num_labels=2，若是多分类问题可通过num_labels参数设置\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# 将模型权重从CPU内存移动到GPU显存\n",
    "model.to(device)\n",
    "# 用于将模型设置为​​训练模式​​。这个调用会改变模型在前向传播和反向传播时的行为\n",
    "model.train()\n",
    "\n",
    "optim = optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86153b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2500/2500 [12:54<00:00,  3.23it/s, epoch=0, train loss=0.2822, train acc=0.88, val loss=0.2362, val acc=0.91, time=774.32]\n",
      "Epoch 1: 100%|██████████| 2500/2500 [12:39<00:00,  3.29it/s, epoch=1, train loss=0.1681, train acc=0.94, val loss=0.2186, val acc=0.92, time=759.89]\n",
      "Epoch 2: 100%|██████████| 2500/2500 [12:42<00:00,  3.28it/s, epoch=2, train loss=0.1050, train acc=0.96, val loss=0.2714, val acc=0.90, time=762.98]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    start = time.time()\n",
    "    train_loss, val_losses = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n",
    "        for batch in train_loader:\n",
    "            n += 1\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "            train_loss += loss.cpu()\n",
    "\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                'train acc': '%.2f' % (train_acc / n)\n",
    "                                })\n",
    "            pbar.update(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                m += 1\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "                val_losses += val_loss\n",
    "        end = time.time()\n",
    "        runtime = end - start\n",
    "        pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                            'train loss': '%.4f' % (train_loss.data / n),\n",
    "                            'train acc': '%.2f' % (train_acc / n),\n",
    "                            'val loss': '%.4f' % (val_losses.data / m),\n",
    "                            'val acc': '%.2f' % (val_acc / m),\n",
    "                            'time': '%.2f' % (runtime)})\n",
    "\n",
    "        # print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n",
    "        #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44c921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predction: 100%|██████████| 1563/1563 [07:41<00:00,  3.38it/s]\n",
      "2025-05-06 23:35:14,791: INFO: result saved!\n"
     ]
    }
   ],
   "source": [
    "test_pred = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc='Predction') as pbar:\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # test_pred.extent\n",
    "            test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "result_output.to_csv(\"./result/bert_native.csv\", index=False, quoting=3)\n",
    "logging.info('result saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
