{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d57db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 使用AutoXXX更智能\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbf57d",
   "metadata": {},
   "source": [
    "hugging face库中对应模型的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52bc2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roberta = \"xlm-roberta-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ee98d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:15:06,346: INFO: running d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py--f=c:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-v350ee38b4198a2c8f534781948c623c65f26c7cd0.json\n"
     ]
    }
   ],
   "source": [
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(r\"running %s\" % ''.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledTrainDataPath = r\"D:\\workplace\\NLP_learning\\dataset\\labeledTrainData.tsv\"\n",
    "testDataPath = r\"D:\\workplace\\NLP_learning\\dataset\\testData.tsv\"\n",
    "\n",
    "def ReadData(path):\n",
    "    return pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
    "train = ReadData(labeledTrainDataPath)\n",
    "test = ReadData(testDataPath)\n",
    "\n",
    "train_texts, train_labels, test_texts = [], [], []\n",
    "for i, review in enumerate(train[\"review\"]):\n",
    "    train_texts.append(review)\n",
    "    train_labels.append(train[\"sentiment\"][i])\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    test_texts.append(review)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9eeee",
   "metadata": {},
   "source": [
    "开始训练Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfeedac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条样本的编码:\n",
      "{'input_ids': [0, 44, 8912, 282, 3395, 5154, 41550, 6921, 9494, 25, 7, 21, 12713, 875, 177283, 136, 3514, 208, 93905, 3357, 2481, 621, 4734, 142, 27992, 5, 20413, 903, 1346, 12, 442, 25, 7, 162520, 764, 831, 25, 18, 27992, 100052, 3267, 248, 74047, 3267, 57433, 68551, 272, 6921, 9494, 4, 77064, 669, 6057, 111, 70, 19336, 49119, 23, 1919, 661, 441, 22153, 15663, 133, 134327, 160964, 90, 1639, 4, 661, 441, 22153, 42724, 7, 1639, 136, 28, 5416, 35801, 223, 661, 441, 22153, 15663, 133, 134327, 1639, 36549, 15, 53927, 1957, 10, 147453, 12768, 3688, 4, 707, 83, 442, 1660, 163, 32, 247, 13950, 7831, 1919, 150679, 9, 21732, 9, 7655, 2242, 5132, 47, 70, 6957, 49119, 678, 661, 441, 22153, 15663, 133, 134327, 12, 173883, 6889, 154162, 1639, 15, 53, 2037, 54926, 2685, 25, 7, 10, 71496, 13, 12768, 111, 34153, 86595, 678, 242, 441, 25, 16, 20, 198343, 142, 50094, 111, 661, 441, 22153, 15663, 133, 134327, 1639, 1163, 5252, 25842, 678, 10, 335, 9, 187430, 100052, 3267, 248, 74047, 3267, 57433, 20320, 10, 29752, 47, 456, 116292, 13, 10, 6957, 99546, 47, 30098, 442, 1295, 8035, 51876, 390, 142, 66110, 19, 62233, 171751, 15, 54084, 85, 6438, 37843, 32673, 247, 41550, 136, 58386, 148503, 621, 220, 15876, 450, 70, 99546, 83, 8035, 28560, 297, 390, 15672, 66631, 7, 15, 2729, 7668, 46840, 282, 136, 195784, 33041, 5084, 247, 1810, 47, 192026, 10, 57491, 99675, 13, 186, 11, 2271, 442, 1556, 68062, 41566, 297, 5, 20255, 442, 103049, 759, 144189, 2481, 47, 5154, 6, 41872, 58, 3957, 53, 25, 107, 98, 10, 173883, 6889, 154162, 678, 3316, 36107, 7432, 41872, 38843, 15, 124409, 144189, 2481, 32, 20, 10462, 20537, 5, 16, 16093, 3267, 248, 74047, 3267, 57433, 3957, 23577, 83, 193, 169429, 4, 237, 442, 83, 41550, 25, 7, 26249, 39411, 8780, 450, 25793, 14794, 70, 1346, 5, 4263, 398, 7413, 1919, 5132, 31577, 4, 70, 1346, 83, 10, 137578, 4, 1284, 2174, 398, 25, 107, 10, 1207, 111, 40101, 4049, 15, 162, 10, 8063, 669, 4, 17467, 56068, 18, 707, 1660, 111719, 20751, 9, 17591, 7710, 16, 707, 1919, 113976, 45831, 4, 41206, 1286, 111, 70, 5701, 98, 10, 38134, 9, 86667, 20334, 100052, 3267, 248, 74047, 3267, 57433, 98385, 6512, 73, 1507, 4, 109208, 7844, 28127, 429, 4, 2965, 29968, 136, 71464, 56, 111, 70, 164098, 15663, 133, 134327, 661, 102250, 1639, 15, 4398, 442, 186, 35839, 450, 678, 10, 80560, 2577, 32, 247, 33022, 7, 136, 8951, 7, 678, 70, 5701, 84773, 68062, 21407, 450, 7228, 41550, 10, 197540, 56409, 24804, 834, 56, 100052, 3267, 248, 74047, 3267, 57433, 3957, 2684, 44423, 449, 43585, 111, 903, 14277, 83, 450, 41550, 15, 3630, 111, 70, 10846, 3395, 100, 136565, 398, 831, 20653, 36802, 70, 1119, 60397, 2320, 26847, 7730, 390, 237, 764, 70424, 7, 16, 136, 148503, 15, 68551, 272, 25, 7, 5732, 4032, 111, 209, 5369, 4, 38781, 329, 271, 6, 25958, 136, 20903, 271, 34391, 16, 85689, 442, 1884, 442, 15744, 1632, 111, 2363, 128745, 3387, 4, 116987, 70, 6, 41872, 58, 160018, 927, 58982, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Roberta)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "print(\"第一条样本的编码:\")\n",
    "print({k: v[0] for k, v in train_encodings.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7aab814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dict.items()返回字典中所有键值对的视图对象，格式为 (key, value) 元组。\n",
    "        # item是一个字典，保存了每个键的第idx个样本\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_samples=0):\n",
    "        self.encodings = encodings\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "train_dataset = TrainDataset(train_encodings, train_labels)\n",
    "val_dataset = TrainDataset(val_encodings, val_labels)\n",
    "test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa141fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(Roberta)\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699fb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2500/2500 [14:11<00:00,  2.94it/s, epoch=0, train loss=0.3009, train acc=0.87, val loss=0.2062, val acc=0.92, time=851.36]\n",
      "Epoch 1: 100%|██████████| 2500/2500 [14:16<00:00,  2.92it/s, epoch=1, train loss=0.1807, train acc=0.93, val loss=0.1991, val acc=0.93, time=856.89]\n",
      "Epoch 2: 100%|██████████| 2500/2500 [14:16<00:00,  2.92it/s, epoch=2, train loss=0.1250, train acc=0.96, val loss=0.2127, val acc=0.92, time=857.09]\n",
      "Predction: 100%|██████████| 3125/3125 [07:36<00:00,  6.84it/s]\n",
      "2025-05-08 14:05:40,334: INFO: result saved!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    start = time.time()\n",
    "    train_loss, val_losses = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n",
    "        for batch in train_loader:\n",
    "            n += 1\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "            train_loss += loss.cpu()\n",
    "\n",
    "            pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                                'train loss': '%.4f' % (train_loss.data / n),\n",
    "                                'train acc': '%.2f' % (train_acc / n)\n",
    "                                })\n",
    "            pbar.update(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                m += 1\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
    "                val_losses += val_loss\n",
    "        end = time.time()\n",
    "        runtime = end - start\n",
    "        pbar.set_postfix({'epoch': '%d' % (epoch),\n",
    "                            'train loss': '%.4f' % (train_loss.data / n),\n",
    "                            'train acc': '%.2f' % (train_acc / n),\n",
    "                            'val loss': '%.4f' % (val_losses.data / m),\n",
    "                            'val acc': '%.2f' % (val_acc / m),\n",
    "                            'time': '%.2f' % (runtime)})\n",
    "\n",
    "        # print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n",
    "        #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n",
    "test_pred = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc='Predction') as pbar:\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # test_pred.extent\n",
    "            test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "result_output.to_csv(\"./result/roberta.csv\", index=False, quoting=3)\n",
    "logging.info('result saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
