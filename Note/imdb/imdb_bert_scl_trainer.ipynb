{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fe0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import losses\n",
    "\n",
    "class BertScratch(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.alpha = 0.2\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        # (batch_size, hidden_size)\n",
    "        pooled_output = outputs[1]\n",
    "    \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # (batch_size, num_labels)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            ce_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "            scl_fct = losses.SupConLoss()\n",
    "            scl_loss = scl_fct(pooled_output, labels)\n",
    "\n",
    "            loss = ce_loss + self.alpha * scl_loss\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054398b",
   "metadata": {},
   "source": [
    "## 自定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0dc26",
   "metadata": {},
   "source": [
    "- 需要继承自PreTrainedModel, 在init函数中向父类传递参数，在forward函数中进行计算\n",
    "- 若结果存在label，那么需要重写forward方法来计算每个逻辑的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96edf8",
   "metadata": {},
   "source": [
    "## BertModel的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6bdbf",
   "metadata": {},
   "source": [
    "1. ​​BERT 模型的输出结构​​\n",
    "\n",
    "BERT 模型的输出是一个元组（或 BaseModelOutput 对象），包含以下内容：\n",
    "- ​outputs[0]​​: 所有 token 的隐藏状态（形状为 (batch_size, sequence_length, hidden_size)），即每个 token 的上下文表示。\n",
    "- ​outputs[1]​​: 池化后的序列表示（形状为 (batch_size, hidden_size)），通常对应 [CLS] token 的隐藏状态经过额外线性层和激活函数（如 tanh）处理后的结果，用于分类任务。\n",
    "- ​​outputs.hidden_states​​: 所有层的隐藏状态（需设置 output_hidden_states=True）\n",
    "- outputs.attentions​​: 注意力权重（需设置 output_attentions=True）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452ab71",
   "metadata": {},
   "source": [
    "2. ​​为什么使用 outputs[1]？​​\n",
    "​\n",
    "- ​分类任务需求​​：在序列分类任务中，通常需要将整个序列的信息压缩为一个固定长度的向量。BERT 的 \n",
    "[CLS] token 的池化输出（outputs[1]）被设计为捕获整个序列的全局信息。\n",
    "- ​与 outputs[0] 的区别​​：\n",
    "  - outputs[0] 包含所有 token 的细粒度表示，适合 token 级任务（如 NER）。\n",
    "  - outputs[1] 是聚合后的表示，适合序列级任务（如文本分类）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06ea3d",
   "metadata": {},
   "source": [
    "3. ​​代码中的具体应用​​\n",
    "在 BertScratch 中：\n",
    "```python\n",
    "pooled_output = outputs[1]  # 提取池化后的表示 (batch_size, hidden_size)\n",
    "pooled_output = self.dropout(pooled_output)\n",
    "logits = self.classifier(pooled_output)  # 分类层输入\n",
    "```\n",
    "- ​pooled_output​​ 作为分类器的输入，通过线性层 (self.classifier) 映射到标签空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cc436",
   "metadata": {},
   "source": [
    "**总结**\n",
    "\n",
    "outputs[1] 是 BERT 为分类任务设计的池化输出，形状为 (batch_size, hidden_size)，而非 token 级的 (batch_size, sequence_length, hidden_size)。这种设计简化了序列级任务的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6b97c",
   "metadata": {},
   "source": [
    "## loss相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d5fa8",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdd06",
   "metadata": {},
   "source": [
    "交叉熵（Cross Entropy）损失是在分类问题中常用的损失函数，尤其在神经网络的训练中经常被使用。它衡量了模型的预测概率分布与实际标签的分布之间的差异\n",
    "\n",
    "二分类交叉熵公式\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "C分类交叉熵公式\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C y_{i,j} \\log(\\hat{y}_{i,j})\n",
    "$$\n",
    "$y_{i,j}$是one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()\n",
    "ce_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431130cc",
   "metadata": {},
   "source": [
    "1. ​​nn.CrossEntropyLoss() 的功能​​\n",
    "- ​作用​​：\n",
    "  该损失函数结合了 Softmax 和负对数似然损失（NLLLoss），用于衡量模型预测的概率分布与真实标签的差异，适用于多分类任务。\n",
    "- ​​输入要求​​：\n",
    "  - ​logits​​：模型的原始输出（未经过 Softmax），形状需为 (batch_size, num_classes）。\n",
    "  - labels​​：真实标签，形状为 (batch_size,)，每个元素是类别的整数索引（如 [0, 2, 1]）。\n",
    "\n",
    "注意，输入要求为logits而不能经过Softmax，形状为（batch_size, num_classes）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8591fd",
   "metadata": {},
   "source": [
    "2. 为什么需要对logits和labels的形状进行调整？\n",
    "- 输入要求​​：nn.CrossEntropyLoss要求logits形状为(N, C)（N是样本数，C是类别数），标签形状为(N,)\n",
    "\n",
    "- 三维logits的场景​​：在token级任务（如命名实体识别、token级情感分析）中，模型对每个token输出分类结果，此时logits形状为(batch_size, seq_len, num_labels)。展平后：\n",
    "  - ​​logits​​：(batch_size * seq_len, num_labels)\n",
    "  - ​labels​​：(batch_size * seq_len,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee30ad0",
   "metadata": {},
   "source": [
    "### SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Yonglong Tian (yonglong@mit.edu)\n",
    "Date: May 07, 2020\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        \n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        \"\"\" \n",
    "            构建mask: mask[i,j] = 1表示两者属于同一label\n",
    "        \"\"\"\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "        \"\"\"\n",
    "            将feature从(batch_size, n_views, hidden_size)转换为(batch_size*n_view, hidden_size)\n",
    "        \"\"\"\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469a439",
   "metadata": {},
   "source": [
    "#### SupConLoss的思想：\n",
    "- 将无监督学习中的对比学习拓展到有监督学习中来：在无监督学习中每个样例只有一个正样本，和其它2N-1个负样本，如果其他2N-1个样本中如果有和目前样本同一类别的样本，也会被视为负样本。在有监督学习的对比学习中：将同属一个类别的视为正样本，其他的为负样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f434b2",
   "metadata": {},
   "source": [
    "### 代码中相关函数小节：\n",
    "- torch.mean(input,*): 返回所有元素的平均值  \n",
    "- torch.eq(input,other): 判断每个相应的元素是否相等，注意第二个参数可以广播\n",
    "- torch.unbind(input, dim=0): 移除张量的一个维度。返回给定维度上所有已去除该维度的切片的元组。\n",
    "  - 假设a.shape = (3,4), 则torch.unbind(a)返回(a[0], a[1], a[2])\n",
    "- torch.cat(tensors, dim=0, *, out=None) → Tensor:在给定维度上连接给定的张量序列。所有张量要么具有相同的形状（除了连接维度），要么是大小为 (0,) 的一维空张量。\n",
    "  - cat不会增加新的维度，但会修改指定的维度，stack会添加新维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc20c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
