{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827c7709",
   "metadata": {},
   "source": [
    "Transformer 的架构为 encoder-decoder neural network。 \n",
    "主要元件有 1.Word embedding 2.Encoder 3.Decoder。\n",
    "1. word embedding： 因为神经网络只能读懂 digit input，所以需要将输入转化为tokens，也就是把语言-数字（向量）。这一步在Quest的前面的视频有，讲得很清楚，可以去看看。下面简单讲讲几点：词嵌入的主要架构为 activation function + positional embedding。\n",
    "   - activation function激活函数（对输入的每个one-hot加权得到一个数值）有几个，每一个词就会有几个embedding value，也就是可以看成一个词会被嵌入到几维的向量。一个模型的激活函数是固定的（激活函数的个数），不随着输入句子改变而改变。这保证了Transformer能够处理不同长度的句子并且高速运行（parallel这点后面会反复提到）\n",
    "   - positional embedding。有很多方法，常用的为sin函数。不妨假设输入字词个数为3，嵌入维度为4。则对应4个振幅越来越大的sin函数，第i个sin函数上取3个点，每个点的sin值就是对应字词第i维的位置嵌入。最后该位置嵌入会加到激活函数得到的嵌入值作为encoder的输入。\n",
    "2. Encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
