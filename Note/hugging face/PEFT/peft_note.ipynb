{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicktour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数高效微调（PEFT Parameter-Efficient Fine-Tuning​​）为大型预训练模型提供了参数高效的微调方法。传统做法是针对每个下游任务对模型的所有参数进行微调，但由于如今模型的参数数量庞大，这种方法的成本越来越高，也越来越不切实际。相反，训练较少的提示参数或使用诸如低秩适应（LoRA low-rank adaptation）之类的重新参数化方法来减少可训练参数的数量会更高效。\n",
    "\n",
    "本快速入门将向您展示 PEFT 的主要功能，以及如何在普通消费者设备上训练或运行通常无法触及的大型模型的推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每种参数高效微调（PEFT）方法都由一个 PeftConfig 类定义，该类存储构建PeftModel所需的所有重要参数。例如，若要使用低秩适应（LoRA）进行训练，请加载并创建一个 LoraConfig 类，并指定以下参数：\n",
    "- task_type: 目标任务\n",
    "- inference_mode：是否使用模型来进行预测\n",
    "- r: 低秩矩阵的维度\n",
    "- lora_alpha：低秩矩阵的缩放因子\n",
    "- lora_dropout：LoRA 层的丢弃概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T08:01:01.567645Z",
     "iopub.status.busy": "2025-05-13T08:01:01.567244Z",
     "iopub.status.idle": "2025-05-13T08:01:01.573187Z",
     "shell.execute_reply": "2025-05-13T08:01:01.572208Z",
     "shell.execute_reply.started": "2025-05-13T08:01:01.567616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦设置好 LoraConfig，就可以使用 get_peft_model() 函数创建一个 PeftModel。该函数需要一个基础模型（可以从 Transformers 库中加载）以及包含用于使用 LoRA 训练模型的参数的 LoraConfig。\n",
    "\n",
    "加载您想要微调的基础模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T08:01:01.575266Z",
     "iopub.status.busy": "2025-05-13T08:01:01.575015Z",
     "iopub.status.idle": "2025-05-13T08:01:02.350185Z",
     "shell.execute_reply": "2025-05-13T08:01:02.349455Z",
     "shell.execute_reply.started": "2025-05-13T08:01:01.575249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"bigscience/mt0-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `get_peft_model()` 函数`model`和 `peft_config` 包装起来以创建一个 `PeftModel`。要了解模型中可训练参数的数量，请使用 `print_trainable_parameters` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T08:01:02.351402Z",
     "iopub.status.busy": "2025-05-13T08:01:02.351107Z",
     "iopub.status.idle": "2025-05-13T08:01:02.600629Z",
     "shell.execute_reply": "2025-05-13T08:01:02.599707Z",
     "shell.execute_reply.started": "2025-05-13T08:01:02.351376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.1915\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 bigscience/mt0-large 的 12 亿参数中，您只需训练其中的 0.19%！\n",
    "\n",
    "就是这样 🎉！现在您可以使用 Transformers Trainer、Accelerate 或任何自定义的 PyTorch 训练循环来训练模型。\n",
    "\n",
    "例如，若要使用 Trainer 类进行训练，请设置一个带有某些训练超参数的 TrainingArguments 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-name/bigscience/mt0-large-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT configuration and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何参数高效微调（PEFT）方法，您都需要创建一个config，其中包含所有指定应如何应用该 PEFT 方法的参数。配置设置完成后，将其与base model一起传递给 get_peft_model() 函数，以创建可训练的 PeftModel。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prompt tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示调优将所有任务都视为生成任务，并在输入中添加特定于任务的提示，该提示会独立更新。`prompt_tuning_init_text` 参数指定了如何微调模型（在本例中，是判断推文是否为投诉）。为了获得最佳效果，`prompt_tuning_init_text` 应该具有与应预测的`token`数量相同的`token`数。为此，您可以将 `num_virtual_tokens` 设置为 `prompt_tuning_init_text` 的标记数。\n",
    "\n",
    "\n",
    "创建一个`PromptTuningConfig`，其中包含任务类型、用于训练模型的初始提示调优文本、要添加和学习的虚拟`token`数量，以及一个`tokenizer`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n",
    "\n",
    "prompt_tuning_init_text = \"Classify if the tweet is a complaint or no complaint.\\n\"\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n",
    "    prompt_tuning_init_text=prompt_tuning_init_text,\n",
    "    tokenizer_name_or_path=\"bigscience/bloomz-560m\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prefix tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前缀调优在模型的所有层中添加特定任务的参数，这些参数由一个单独的前馈网络进行优化。使用任务类型和要添加及学习的虚拟token数量创建一个 PrefixTuningConfig 配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "\n",
    "peft_config = PrefixTuningConfig(task_type=\"CAUSAL_LM\", num_virtual_tokens=20)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\"trainable params: 983,040 || all params: 560,197,632 || trainable%: 0.1754809274167014\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**p-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P 调优添加了一个可训练的嵌入张量，提示token可以在输入序列中的任何位置添加。使用任务类型、要添加和学习的虚拟token数量以及用于学习提示参数的编码器隐藏大小创建一个 PromptEncoderConfig。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PromptEncoderConfig, get_peft_model\n",
    "\n",
    "peft_config = PromptEncoderConfig(\n",
    "    task_type=\"CAUSAL_LM\", \n",
    "    num_virtual_tokens=20, \n",
    "    encoder_hidden_size=128\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
