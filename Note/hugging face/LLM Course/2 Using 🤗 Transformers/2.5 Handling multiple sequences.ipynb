{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3891d3e",
   "metadata": {},
   "source": [
    "### Models expect a batch of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e0d60",
   "metadata": {},
   "source": [
    "model期望按照batched收到input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ecd02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "# model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ace6c",
   "metadata": {},
   "source": [
    "观察可以发现 tokenizer 的输出比我们之前的输入要多出一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c253ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec59de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "# This line will fail.\n",
    "output = model(input_ids)\n",
    "\n",
    "print(output[\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96216e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "input_ids = torch.tensor(batched_ids)\n",
    "output = model(input_ids)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcba2cd",
   "metadata": {},
   "source": [
    "Padding the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937f6d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m batched_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      3\u001b[0m     [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m]\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "torch.tensor(batched_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93389c57",
   "metadata": {},
   "source": [
    "不规则的 list of list 无法转换为tensor。因此我们会对token少的sentence进行填充，使用 padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38be224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3f144",
   "metadata": {},
   "source": [
    "使用 padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c68052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "seq1_ids = [[200,200,200]]\n",
    "seq2_ids = [[200,200]]\n",
    "bached_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200,tokenizer.pad_token_id]\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(seq1_ids)))\n",
    "print(model(torch.tensor(seq2_ids)))\n",
    "print(model(torch.tensor(bached_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5984618",
   "metadata": {},
   "source": [
    "我们发现 seq2 的输出和 bached_ids 中的第二条句子的输出不一致。这是因为模型的attention layer把padding token也当作了上下文的一部分。因此我们需要使用 attention mask来告诉attention哪些token是不用考虑的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1290891",
   "metadata": {},
   "source": [
    "### Attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4cf97",
   "metadata": {},
   "source": [
    "attention mask是和input IDs有一样形状的张量。每个元素由0和1组成，代表了是否需要被attention考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23c48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "output = model(torch.tensor(bached_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(output['logits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a1e49",
   "metadata": {},
   "source": [
    " Try it out! Apply the tokenization manually on the two sentences used in section 2 (“I’ve been waiting for a HuggingFace course my whole life.” and “I hate this so much!”). Pass them through the model and check that you get the same logits as in section 2. Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a17243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子1的token IDs: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "句子2的token IDs: [1045, 5223, 2023, 2061, 2172, 999]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\"\n",
    "]\n",
    "# 手动tokenize（add_special_tokens=False是为了更清晰对比）\n",
    "seq1_ids = tokenizer.encode(sentences[0], add_special_tokens=False)\n",
    "seq2_ids = tokenizer.encode(sentences[1], add_special_tokens=False)\n",
    "\n",
    "print(\"句子1的token IDs:\", seq1_ids)\n",
    "print(\"句子2的token IDs:\", seq2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44b108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "手动构造的input_ids:\n",
      " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012],\n",
      "        [ 1045,  5223,  2023,  2061,  2172,   999,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "手动构造的attention_mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 计算最大长度\n",
    "max_len = max(len(seq1_ids), len(seq2_ids))\n",
    "\n",
    "# 对短句进行右侧填充\n",
    "seq1_ids = seq1_ids + [tokenizer.pad_token_id] * (max_len - len(seq1_ids))\n",
    "seq2_ids = seq2_ids + [tokenizer.pad_token_id] * (max_len - len(seq2_ids))\n",
    "\n",
    "# 构造输入张量\n",
    "input_ids = torch.tensor([seq1_ids, seq2_ids])\n",
    "\n",
    "# 生成attention mask（padding位置为0）\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).int()\n",
    "\n",
    "print(\"\\n手动构造的input_ids:\\n\", input_ids)\n",
    "print(\"手动构造的attention_mask:\\n\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0a035fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
      "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model(input_ids, attention_mask=attention_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
